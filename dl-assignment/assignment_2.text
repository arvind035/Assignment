1. Describe the structure of an artificial neuron. How is it similar to a biological neuron? What are its main components?

    Artificial Neuron Structure: It consists of inputs, weights, a summation function, an activation function, and an output.
    Similarity to Biological Neuron: Both receive inputs (dendrites in biological, features in artificial), apply a function (summation), and produce an output (firing or not firing).
    Main Components: Inputs, weights, summation, bias, activation function, and output.

2. What are the different types of activation functions popularly used? Explain each of them.

    Sigmoid Function: Maps input values to the range (0, 1), often used in binary classification.
    Tanh Function: Maps inputs to the range (-1, 1); commonly used in hidden layers.
    ReLU (Rectified Linear Unit): Outputs the input if positive; otherwise, outputs 0, used in deep networks.
    Leaky ReLU: Similar to ReLU, but allows a small negative gradient for negative inputs to solve the dying ReLU problem.
    Softmax: Converts outputs into probabilities, useful in multi-class classification.

3.1 Explain, in detail, Rosenblatt’s perceptron model. How can a set of data be classified using a simple perceptron?

    Rosenblatt’s Perceptron: A binary classifier that computes a weighted sum of inputs and applies a step function to classify data as either 0 or 1 based on a threshold.
    Classification Process: Inputs are weighted and summed, passed through a step function, and classified depending on whether the result exceeds the threshold.

3.2 Use a simple perceptron with weights w0, w1, and w2 as −1, 2, and 1, respectively, to classify data points (3, 4); (5, 2); (1, −3); (−8, −3); (−3, 0).

    Classification Process:
        For (3, 4): Result = -1 + 2(3) + 1(4) = 9 → Class 1.
        For (5, 2): Result = -1 + 2(5) + 1(2) = 11 → Class 1.
        For (1, -3): Result = -1 + 2(1) + 1(-3) = -2 → Class 0.
        For (-8, -3): Result = -1 + 2(-8) + 1(-3) = -20 → Class 0.
        For (-3, 0): Result = -1 + 2(-3) + 1(0) = -7 → Class 0.

4. Explain the basic structure of a multi-layer perceptron. Explain how it can solve the XOR problem.

    MLP Structure: Consists of an input layer, hidden layer(s), and an output layer with non-linear activation functions.
    XOR Solution: The hidden layer allows MLP to capture non-linear relationships, making it possible to solve linearly inseparable problems like XOR.

5. What is artificial neural network (ANN)? Explain some of the salient highlights in the different architectural options for ANN.

    ANN Definition: A computational model inspired by biological neural networks, consisting of interconnected artificial neurons.
    Architectural Highlights: Single-layer, multi-layer (MLP), convolutional (CNN), recurrent (RNN), and deep neural networks (DNN) architectures each serve specific purposes in handling complexity and different types of data.

6. Explain the learning process of an ANN. Explain, with example, the challenge in assigning synaptic weights for the interconnection between neurons? How can this challenge be addressed?

    Learning Process: Weights are adjusted iteratively based on error feedback using gradient descent to minimize the loss function.
    Challenge in Assigning Weights: Initial weight assignment can lead to slow learning or convergence issues, addressed by proper weight initialization (e.g., Xavier initialization) and techniques like momentum.

7. Explain, in detail, the backpropagation algorithm. What are the limitations of this algorithm?

    Backpropagation: A method to calculate the gradient of the loss function with respect to the weights by propagating the error backward through the network.
    Limitations: Slow convergence, risk of vanishing gradients in deep networks, and sensitivity to initial weights.

8. Describe, in detail, the process of adjusting the interconnection weights in a multi-layer neural network.

    Weight Adjustment Process: Weights are updated using gradient descent based on the computed error from the output layer back through hidden layers, adjusting weights by the learning rate and error gradient.

9. What are the steps in the backpropagation algorithm? Why is a multi-layer neural network required?

    Backpropagation Steps:
        Forward pass to compute the output.
        Calculate error.
        Backward pass to compute gradients.
        Update weights.
    Need for Multi-layer Network: Single-layer perceptrons cannot solve non-linear problems, but multi-layer networks can.

10. Write short notes on:

1. Artificial Neuron:

    A mathematical model that mimics the function of a biological neuron, computing a weighted sum of inputs, applying an activation function, and generating an output.

2. Multi-layer Perceptron:

    A type of ANN with multiple layers (input, hidden, and output) that can model complex non-linear relationships.

3. Deep Learning:

    A subfield of machine learning focused on neural networks with many layers, allowing for high-level feature learning and improved performance on large, complex datasets.

4. Learning Rate:

    A hyperparameter controlling how much to change the model's weights in response to the error during training. A high rate may overshoot the optimal solution, while a low rate may slow down training.

11. Write the difference between:

1. Activation function vs Threshold function:

    Activation Function: Continuous or non-linear function like sigmoid, ReLU, or tanh.
    Threshold Function: Binary function that outputs 1 if input exceeds a threshold, otherwise 0.

2. Step function vs Sigmoid function:

    Step Function: A binary, non-continuous function.
    Sigmoid Function: A smooth, continuous function mapping inputs between 0 and 1.

3. Single layer vs Multi-layer Perceptron:

    Single Layer Perceptron: Can solve only linearly separable problems.
    Multi-layer Perceptron: Can solve both linear and non-linear problems using hidden layers.


