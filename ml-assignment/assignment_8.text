1. What exactly is a feature? Give an example to illustrate your point.

    Feature Definition: A feature is an individual measurable property or characteristic of a phenomenon being observed.
    Example: In a dataset for predicting house prices, features could include the number of bedrooms, square footage, and location.

2. What are the various circumstances in which feature construction is required?

    When original features do not adequately represent the data.
    To improve model performance by creating new variables that capture important relationships.
    When combining existing features into more meaningful aggregates (e.g., total income from individual income sources).

3. Describe how nominal variables are encoded.

    Encoding Methods:
        One-Hot Encoding: Creates binary columns for each category (e.g., color: red, green, blue becomes three columns).
        Label Encoding: Assigns a unique integer to each category (e.g., red = 0, green = 1, blue = 2).

4. Describe how numeric features are converted to categorical features.

    Binning: Numeric values are grouped into discrete categories based on defined intervals (e.g., age ranges: 0-18, 19-35, 36-50).
    Thresholding: Assigns categories based on whether the numeric value exceeds a certain threshold (e.g., income: low, medium, high).

5. Describe the feature selection wrapper approach. State the advantages and disadvantages of this approach.

    Wrapper Approach: Uses a predictive model to evaluate the combination of features. It adds or removes features based on the model's performance.
        Advantages:
            Tailored to specific models; can lead to better performance.
            Considers feature interactions.
        Disadvantages:
            Computationally expensive due to repeated model training.
            Prone to overfitting on the training data.

6. When is a feature considered irrelevant? What can be said to quantify it?

    A feature is considered irrelevant if it does not provide any useful information for predicting the target variable.
    Quantification: Statistical tests (e.g., chi-square test, correlation coefficients) can be used to assess the relevance of features.

7. When is a function considered redundant? What criteria are used to identify features that could be redundant?

    A function is considered redundant if it does not add any additional information beyond what is already provided by other features.
    Criteria:
        High correlation with another feature.
        Similar predictive power as another feature when included in the model.

8. What are the various distance measurements used to determine feature similarity?

    Common Distance Measurements:
        Euclidean Distance: Straight-line distance between two points.
        Manhattan Distance: Sum of absolute differences between points.
        Cosine Similarity: Measures the cosine of the angle between two vectors.
        Minkowski Distance: Generalized distance that includes both Euclidean and Manhattan distances as special cases.

9. State the difference between Euclidean and Manhattan distances.

    Euclidean Distance: Measures the shortest straight-line distance between two points in space.
    Manhattan Distance: Measures the total distance traveled along axes at right angles (grid-like path).
    Formula Difference:
        Euclidean: d=∑(xi−yi)2d=∑(xi​−yi​)2

        ​
        Manhattan: d=∑∣xi−yi∣d=∑∣xi​−yi​∣

10. Distinguish between feature transformation and feature selection.

    Feature Transformation: Involves modifying existing features to create new ones (e.g., normalization, PCA).
    Feature Selection: Involves selecting a subset of existing features based on their relevance or contribution to the model's performance.

11. Make brief notes on any two of the following:

    SVD (Singular Value Decomposition):
        A matrix factorization technique that decomposes a matrix into three components, providing insights into the structure and dimensionality of data. It’s commonly used in recommendation systems and latent semantic analysis.

    Receiver Operating Characteristic (ROC) Curve:
        A graphical representation of a classifier's performance, plotting true positive rates against false positive rates at various threshold settings. The area under the ROC curve (AUC) quantifies the overall performance; a higher AUC indicates better model performance.
