Answers

1. What is the COVARIATE SHIFT Issue, and how does it affect you?

    Covariate Shift: This occurs when the distribution of input data changes between training and testing phases, while the relationship between inputs and outputs remains the same.
    Impact: It can lead to poor model performance since the model is trained on data that may not represent the actual data encountered in real-world applications.

2. What is the process of BATCH NORMALIZATION?

    Batch Normalization Process:
        Calculate the mean and variance of each mini-batch during training.
        Normalize the inputs by subtracting the mean and dividing by the square root of the variance.
        Scale and shift the normalized values using learnable parameters (gamma and beta).
        This helps stabilize and accelerate training by reducing internal covariate shift.

3. Using our own terms and diagrams, explain LENET ARCHITECTURE.

    LeNet Architecture: A convolutional neural network designed for digit recognition. It consists of:
        Input Layer: 32x32 grayscale images.
        Convolutional Layer (C1): 6 filters of size 5x5, resulting in 28x28 feature maps.
        Pooling Layer (S2): Subsampling using average pooling to reduce dimensionality.
        Convolutional Layer (C3): 16 filters of size 5x5 applied to pooled feature maps.
        Pooling Layer (S4): Another subsampling layer.
        Fully Connected Layer (C5): Flattened output connected to a fully connected layer with 120 neurons.
        Output Layer (F6): Final output layer with 10 neurons for digit classification.

4. Using our own terms and diagrams, explain ALEXNET ARCHITECTURE.

    AlexNet Architecture: A deep convolutional neural network known for its success in the ImageNet competition. It consists of:
        Input Layer: 224x224 RGB images.
        Convolutional Layers (Conv1 - Conv5): 5 convolutional layers with varying filter sizes (11x11, 5x5, etc.) and ReLU activations.
        Pooling Layers: Max pooling layers following certain convolutional layers to reduce dimensions.
        Dropout Layers: Used after some fully connected layers to prevent overfitting.
        Fully Connected Layers (FC6, FC7): Two dense layers leading to the output layer.
        Output Layer: 1000 neurons corresponding to the 1000 classes in ImageNet.

5. Describe the vanishing gradient problem.

    Vanishing Gradient Problem: This occurs in deep neural networks when gradients of the loss function approach zero as they are propagated back through layers. This results in minimal updates to weights, hindering learning, especially in the early layers.

6. What is NORMALIZATION OF LOCAL RESPONSE?

    Normalization of Local Response: A technique used in some CNNs to normalize the output of neurons within a local region. This enhances feature robustness and helps in modeling competitive interactions among neurons, often applied after convolutional layers.

7. In AlexNet, what WEIGHT REGULARIZATION was used?

    Weight Regularization: AlexNet employed L2 regularization (also known as weight decay) to reduce overfitting by penalizing large weights during training.

8. Using our own terms and diagrams, explain VGGNET ARCHITECTURE.

    VGGNet Architecture: Known for its simplicity and depth, VGGNet uses:
        Input Layer: 224x224 RGB images.
        Convolutional Layers: 16-19 layers with 3x3 filters and ReLU activation functions.
        Pooling Layers: Max pooling layers after every few convolutional layers to downsample the feature maps.
        Fully Connected Layers: Two or three dense layers leading to the output.
        Output Layer: A layer with 1000 neurons for classification tasks.

9. Describe VGGNET CONFIGURATIONS.

    VGGNet Configurations: VGGNet architectures vary based on depth:
        VGG16: 16 weight layers (13 convolutional and 3 fully connected).
        VGG19: 19 weight layers (16 convolutional and 3 fully connected).
        Consistently uses 3x3 convolutional filters and 2x2 max pooling layers.

10. What regularization methods are used in VGGNET to prevent overfitting?

    Regularization Methods in VGGNet:
        Dropout: Applied to fully connected layers to randomly deactivate a subset of neurons during training.
        Weight Decay (L2 Regularization): Reduces the magnitude of weights to mitigate overfitting.
