Answers

1. Describe the Quick R-CNN architecture.

    Definition: Quick R-CNN is an advancement of Fast R-CNN that utilizes a single-stage end-to-end training process to streamline the training and inference pipeline.
    Architecture: It integrates the Region Proposal Network (RPN) directly into the Fast R-CNN framework, allowing the model to generate region proposals alongside classification and bounding box regression tasks simultaneously.

2. Describe two Fast R-CNN loss functions.

    1. Classification Loss (Softmax Loss): Measures the difference between predicted and true class labels for each region of interest (RoI), typically using softmax to normalize class scores.
    2. Bounding Box Regression Loss (Smooth L1 Loss): Computes the error between the predicted bounding box coordinates and the ground truth, allowing the model to fine-tune box predictions for better localization.

3. Describe the DISABILITIES OF FAST R-CNN.

    1. Dependency on External Region Proposals: Fast R-CNN relies on external region proposal methods (like Selective Search), which can be slow and inefficient.
    2. Limited to Single Scale: It is often limited to detecting objects at a single scale, making it less effective for images containing objects of varying sizes.
    3. Inference Speed: Although faster than traditional R-CNN, it can still be slow during inference due to the processing of multiple proposals.

4. Describe how the area proposal network works.

    Function: The Area Proposal Network (RPN) is a fully convolutional network that simultaneously predicts object bounding boxes and objectness scores for each location in the feature map.
    Process: It uses anchor boxes of different scales and aspect ratios to propose regions likely to contain objects, which are then refined and filtered based on their scores.

5. Describe how the RoI pooling layer works.

    Purpose: The RoI (Region of Interest) pooling layer extracts fixed-size feature maps from varying sized proposals, allowing the model to process different region sizes uniformly.
    Mechanism: It divides each proposal into a fixed grid, pooling features within each grid cell, often using max pooling to retain the most salient features.

6. What are fully convolutional networks and how do they work? (FCNs)

    Definition: Fully Convolutional Networks (FCNs) are neural networks where all layers are convolutional, designed to handle input of arbitrary size and produce output with spatial dimensions.
    Functionality: They replace fully connected layers with convolutional layers, allowing for end-to-end training and making them suitable for tasks like semantic segmentation.

7. What are anchor boxes and how do you use them?

    Definition: Anchor boxes are predefined bounding boxes of various sizes and aspect ratios used as references for predicting object locations in images.
    Usage: They are placed at each spatial position on the feature map, allowing the network to predict offsets to these anchors to generate accurate bounding boxes for detected objects.

8. Describe the Single-shot Detector's architecture (SSD).

    Structure: The SSD architecture consists of a base network (like VGG16) followed by several convolutional layers that detect objects at multiple scales.
    Functionality: It generates predictions for object classes and bounding boxes at different feature map resolutions, enabling it to detect objects of varying sizes efficiently.

9. HOW DOES THE SSD NETWORK PREDICT?

    Process: The SSD network predicts bounding box coordinates and class scores for each anchor box at multiple feature map scales, utilizing convolutional layers to generate these predictions in a single forward pass.
    Output: The final output includes a set of bounding boxes with their corresponding confidence scores for each object class detected within those boxes.

10. Explain Multi Scale Detections?

    Definition: Multi-scale detections refer to the ability of an object detection model to identify objects of varying sizes using feature maps of different resolutions.
    Importance: This capability enhances the model's performance in detecting small and large objects within the same image by providing rich contextual information across scales.

11. What are dilated (or atrous) convolutions?

    Definition: Dilated convolutions, also known as atrous convolutions, are a type of convolution where the kernel is applied over an area larger than its length by skipping input values with a specified "dilation rate."
    Function: This allows for an increased receptive field without increasing the number of parameters, which is beneficial for tasks requiring broader context, like semantic segmentation
